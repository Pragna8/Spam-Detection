# Importing necessary libraries for EDA
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import string
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
nltk.download('stopwords')

# Importing libraries necessary for Model Building and Training
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

import warnings
warnings.filterwarnings('ignore')

# Loading the dataset
email_data = pd.read_csv('Emails.csv')
email_data.head()

# Checking the shape of the dataset
email_data.shape

# Plotting the distribution of spam and non-spam emails
sns.countplot(x='spam', data=email_data)
plt.show()

# Downsampling to balance the dataset
ham_emails = email_data[email_data.spam == 0]
spam_emails = email_data[email_data.spam == 1]
ham_emails = ham_emails.sample(n=len(spam_emails), random_state=42)

# Combining ham and spam emails after downsampling
balanced_emails = ham_emails.append(spam_emails).reset_index(drop=True)
plt.figure(figsize=(8, 6))
sns.countplot(data=balanced_emails, x='spam')
plt.title('Distribution of Ham and Spam email messages after downsampling')
plt.xlabel('Message types')

# Removing 'Subject' from email texts
balanced_emails['text'] = balanced_emails['text'].str.replace('Subject', '')
balanced_emails.head()

# Function to remove stopwords from text
def remove_stopwords(text):
    stop_words = stopwords.words('english')
    important_words = []

    # Storing the important words
    for word in str(text).split():
        word = word.lower()
        if word not in stop_words:
            important_words.append(word)

    return " ".join(important_words)

# Applying stopwords removal to the email text
balanced_emails['text'] = balanced_emails['text'].apply(lambda text: remove_stopwords(text))
balanced_emails.head()

# Function to plot word cloud
def plot_word_cloud(data, email_type):
    email_corpus = " ".join(data['text'])
    plt.figure(figsize=(7, 7))
    wordcloud = WordCloud(background_color='black', max_words=100, width=800, height=400, collocations=False).generate(email_corpus)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'WordCloud for {email_type} emails', fontsize=15)
    plt.axis('off')
    plt.show()

# Plotting word cloud for non-spam and spam emails
plot_word_cloud(balanced_emails[balanced_emails['spam'] == 0], email_type='Non-Spam')
plot_word_cloud(balanced_emails[balanced_emails['spam'] == 1], email_type='Spam')

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(balanced_emails['text'], balanced_emails['spam'], test_size=0.2, random_state=42)

# Tokenize the text data
text_tokenizer = Tokenizer()
text_tokenizer.fit_on_texts(X_train)

# Convert text to sequences
train_sequences = text_tokenizer.texts_to_sequences(X_train)
test_sequences = text_tokenizer.texts_to_sequences(X_test)

# Pad sequences to have the same length
max_sequence_length = 100 # maximum sequence length
train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')
test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')

# Build the model
spam_model = tf.keras.models.Sequential()
spam_model.add(tf.keras.layers.Embedding(input_dim=len(text_tokenizer.word_index) + 1, output_dim=32, input_length=max_sequence_length))
spam_model.add(tf.keras.layers.LSTM(16))
spam_model.add(tf.keras.layers.Dense(32, activation='relu'))
spam_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

# Print the model summary
spam_model.summary()

# Compile the model
spam_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'], optimizer='adam')

# Callbacks for early stopping and learning rate reduction
early_stopping = EarlyStopping(patience=3, monitor='val_accuracy', restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(patience=2, monitor='val_loss', factor=0.5, verbose=0)

# Train the model
training_history = spam_model.fit(train_sequences, y_train, validation_data=(test_sequences, y_test), epochs=20, batch_size=32, callbacks=[reduce_lr, early_stopping])

# Evaluate the model
test_loss, test_accuracy = spam_model.evaluate(test_sequences, y_test)
print('Test Loss:', test_loss)
print('Test Accuracy:', test_accuracy)

# Plotting training and validation accuracy
plt.plot(training_history.history['accuracy'], label='Training Accuracy')
plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()
